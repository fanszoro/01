#梯度下降法:batch gradient decent
 #获得预估值
def h_get(x,theta):
    h=np.dot(x,theta) 
    return h
 #获得批次梯度值
def gradientdecent(x,y,theta):
    df=np.dot((h_get(x,theta)-y).T,x)
    return df
 #获得归一化数据
def normalization(x):
    x_n=[]
    for i in x:
        x_n.append((i-min(x))/(np.max(x)-np.min(x)))
    return x_n
#初始处理数据：分别得到x:x0:(1,1,...,1):x1:buses,y:pgdp,theta:初始参数
x0=np.ones(12)
x1=np.array(data.Bus)
x=np.vstack((x0,x1)).transpose()
y=data.PGDP
theta=[1,1]
print(x,y)
print(gradientdecent(x,y,theta))
lr=[0.01,0.0005]
#for i in range(1000):
#    df=gradientdcent(x,y,theta)
